{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from scipy.cluster.vq import vq\n",
    "from math import pi, floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(source_path):\n",
    "    \"\"\"\n",
    "    Generate images from a given source location, one at a time together with\n",
    "    their class label.\n",
    "    @param source_path: string for the directory to look for images from\n",
    "    @return: yields a pair image, class_name (a string)\n",
    "    \"\"\"\n",
    "    for directory in os.listdir(source_path):\n",
    "        # deal with an unpleasant feature of Mac OS X\n",
    "        if directory == \".DS_Store\":\n",
    "            continue\n",
    "        for image in os.listdir(source_path + \"/\" + directory):\n",
    "            # deal with an unpleasant feature of Mac OS X\n",
    "            if image == \".DS_Store\":\n",
    "                continue\n",
    "            yield cv2.imread(source_path + \"/\" + directory + \"/\" + image), directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(traindata_path):\n",
    "    \"\"\"\n",
    "    Compute and extract descriptors from a set of images.\n",
    "    @param traindata_path: the path to the training images\n",
    "    @return: a pandas DataFrame with all necessary data\n",
    "    \"\"\"\n",
    "    # initialize data structures, including SIFT object\n",
    "    descriptors = pd.DataFrame(columns=[\"data\", \"image\", \"coords\"])\n",
    "    data = list()\n",
    "    images = list()\n",
    "    coords = list()\n",
    "    labels = dict()\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    count = 0\n",
    "    \n",
    "    # for each train image\n",
    "    for image, class_name in tqdm(image_generator(traindata_path)):\n",
    "        # extract keypoints and descriptors\n",
    "        kp, des = sift.detectAndCompute(image, None)\n",
    "        # update lists\n",
    "        for num in range(len(des)):\n",
    "            data.append(des[num])\n",
    "            images.append(count)\n",
    "            coords.append((kp[num].pt[1], kp[num].pt[0]))  # to be used with the spatial pyramid kernel\n",
    "        labels[count] = classes[class_name]\n",
    "        count += 1\n",
    "        \n",
    "    descriptors[\"data\"] = data\n",
    "    descriptors[\"image\"] = images\n",
    "    descriptors[\"coords\"] = coords\n",
    "\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visual_words(descriptors, k, n_sampled, num_descriptors):\n",
    "    \"\"\"\n",
    "    Cluster the a sample of the descriptors into visual words.\n",
    "    @param descriptors: a pandas DataFrame with all the descriptors\n",
    "    @param k: the number of clusters\n",
    "    @param n_sampled: the number of descriptors to sample\n",
    "    @param num_descriptors: total number of descriptors\n",
    "    @return: an object of type sklearn.cluster.KMea\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=k, n_init=5, n_jobs=-1)\n",
    "    kmeans = kmeans.fit(np.array(descriptors[\"data\"].sample(frac=n_sampled / num_descriptors).tolist()))\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histograms(descriptors, count, vocabulary):\n",
    "    \"\"\"\n",
    "    Compute the BoW histograms for all of the training images.\n",
    "    @param descriptors: a pandas DataFrame containing the extracted\n",
    "    descriptors\n",
    "    @param count: the total number of training images\n",
    "    @param vocabulary: the visual words\n",
    "    @return: a NumPy containing the histograms, of size[n_samples, n_visual_words]\n",
    "    \"\"\"\n",
    "    # get number of visual words and allocate matrix\n",
    "    k = len(vocabulary)\n",
    "    histograms = np.zeros((count, k))\n",
    "    \n",
    "    # for each training image\n",
    "    for image in descriptors[\"image\"].unique():\n",
    "        # retrieve its descriptors\n",
    "        desc = np.array([des for des in descriptors[descriptors[\"image\"] == image][\"data\"]])\n",
    "        # quantize them into visual words and update the array\n",
    "        words, _ = vq(desc, vocabulary, check_finite=False)\n",
    "        histograms[image], _ = np.histogram(words, bins=np.arange(k + 1))\n",
    "        \n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histEMD(hist1, hist2, k=k):\n",
    "    \"\"\"\n",
    "    Compute the Earth Mover's Distance between two histograms.\n",
    "    @param hist1: the first histogram\n",
    "    @param hist2: the second histogram\n",
    "    @param k: the number of elements to match per histogram\n",
    "    @return: a float for the Earth Mover's Distance\n",
    "    \"\"\"\n",
    "    # the OpenCV implementation of the EMD requires two 2xn arrays as input,\n",
    "    # where the first column are the weights and the second are the 1-d coordinates\n",
    "    hist1 = np.array([hist1, [num for num in range(k)]], dtype=np.float32).T\n",
    "    hist2 = np.array([hist2, [num for num in range(k)]], dtype=np.float32).T\n",
    "    D, _, _ = cv2.EMD(hist1, hist2, cv2.DIST_L2)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_intersection(hist1, hist2):\n",
    "    \"\"\"\n",
    "    Compute the histogram intersection distance between two histograms.\n",
    "    @param hist1: the firt histogram\n",
    "    @param hist2: the second histogram\n",
    "    @return: a scalar NumPy array with the sum of the minima taken bin-wise\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(np.min(np.array([hist1, hist2]), axis=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisquared_distance(hist1, hist2, k=k):\n",
    "    \"\"\"\n",
    "    Compute the generalized Gaussian kernel based on the Chi-square distance\n",
    "    between two histograms.\n",
    "    @param hist1: the first histogram\n",
    "    @param hist2: the second histogram\n",
    "    @param k: the number of visual words\n",
    "    @return: a NumPy array with the scalar result\n",
    "    \"\"\"\n",
    "    # since in the Chi-squared distance we compute ti - ti', square it, then divide by ti + ti', we might\n",
    "    # encounter a 0/0 or x/0 issue. Luckily, since the tis are non-negative (being the histogram counts), this\n",
    "    # happens if and only if both ti and ti' are equal to 0. Then, the following line scans the histograms\n",
    "    # and stores the indexes of the \"valid\" bins, to be used as masks in the computation of the term D\n",
    "    indexes = np.array([num for num in range(k) if not (hist1[num] == 0 and hist2[num] == 0)])\n",
    "    D = np.sum(np.square(hist1[indexes] - hist2[indexes]) / (hist1[indexes] + hist2[indexes])) # the Chi-squared distance\n",
    "    # plug into the generalized Gaussian kernel\n",
    "    return 0.5 * D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisquared_kernel(hist1, hist2, k, A):\n",
    "    \"\"\"\n",
    "    Compute the generalized Gaussian kernel based on the Chi-square distance\n",
    "    between two histograms.\n",
    "    @param hist1: the first histogram\n",
    "    @param hist2: the second histogram\n",
    "    @param k: the number of visual words\n",
    "    @param A: the scale parameter (> 0)\n",
    "    @return: a NumPy array with the scalar result\n",
    "    \"\"\"\n",
    "    # since in the Chi-squared distance we compute ti - ti', square it, then divide by ti + ti', we might\n",
    "    # encounter a 0/0 or x/0 issue. Luckily, since the tis are non-negative (being the histogram counts), this\n",
    "    # happens if and only if both ti and ti' are equal to 0. Then, the following line scans the histograms\n",
    "    # and stores the indexes of the \"valid\" bins, to be used as masks in the computation of the term D\n",
    "    indexes = np.array([num for num in range(k) if not (hist1[num] == 0 and hist2[num] == 0)])\n",
    "    D = np.sum(np.square(hist1[indexes] - hist2[indexes]) / (hist1[indexes] + hist2[indexes])) # the Chi-squared distance\n",
    "    # plug into the generalized Gaussian kernel\n",
    "    return np.exp(- (D * 0.5) / A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Gram(data, kernel, *args):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix associated to a dataset, from\n",
    "    a predefined kernel function.\n",
    "    @param data: the dataset under consideration, of size [n_samples, n_features]\n",
    "    @param distance_function: the kernel used. Takes as minimal input two rows of 'data'\n",
    "    @param *args: additional positional arguments to be passed to the kernel\n",
    "    @return: the (symmetric) Gram matrix, of size [n_samples, n_samples]\n",
    "    \"\"\"\n",
    "    num_samples, _ = np.shape(data)\n",
    "    # pre-allocate matrix\n",
    "    distance_matrix = np.zeros((num_samples, num_samples))\n",
    "    # since the matrix is symmetric, we just need to fill half of it, then\n",
    "    # copy on the other side\n",
    "    for row in range(num_samples):\n",
    "        for col in range(row, num_samples):\n",
    "            temp = kernel(data[row, :], data[col, :], *args)\n",
    "            distance_matrix[row, col] = temp\n",
    "            distance_matrix[col, row] = temp\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_kernel(descriptor, word, const, sigma_2):\n",
    "    \"\"\"\n",
    "    Compute the likelihood of a descriptor belonging to a word, by kernel density \n",
    "    estimation using a Gaussian kernel with Euclidean distance, centered on the visual words. Used\n",
    "    to compute a soft-assignment \"histogram\". See Van Gemert (2008).\n",
    "    @param descriptor: the 128-dimensional SIFT descriptor under consideration\n",
    "    @param word: the 128-dimensional visual word from the vocabulary under consideration\n",
    "    @param const: the constant (for given std) appearing in front of the Gaussian density.\n",
    "    Pre-computed for efficiency reasons\n",
    "    @param sigma_2: the variance of the Gaussian kernel (aka shape). Fixed for given std, can then be pre-computed\n",
    "    for efficiency reasons\n",
    "    @return: the value of the proposed kernel between the descriptor and the word, a scalar NumPy array\n",
    "    \"\"\"\n",
    "    return const * np.exp(- np.linalg.norm(descriptor - word)**2 / (2 * sigma_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict_knn(clf, sift, vocabulary, train_data, labels, testdata_path, classes):\n",
    "    \"\"\"\n",
    "    Fit a classifier object on a train set and formulate predictions for images in a test set.\n",
    "    To be used with the k-neighbors classifier.\n",
    "    @param clf: a classifier object implementing \"fit\" and \"predict\" methods, like any classifier\n",
    "    conforming to the Scikit-Learn API\n",
    "    @param sift: an object of type cv2.xfeatures2d_SIFT, to extract the SIFT descriptors\n",
    "    @param vocabulary: a NumPy array containing the visual words of the vocabulary\n",
    "    @param train_data: a NumPy array for the train dataset, of size [n_samples, n_features]\n",
    "    @param labels: a dictionary mapping the training images' numbers to their respective class label\n",
    "    @param testdata_path: a string for the directory containing the test images to evaluate\n",
    "    @param classes: a dictionary of class_string: class_number pairs\n",
    "    @return: a tuple (y_true, y_pred), the former being the list of true labels, the latter being the \n",
    "    list of predicted labels\n",
    "    \"\"\"\n",
    "    # allocate lists\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "    # get the number of visual words\n",
    "    k = len(vocabulary)\n",
    "    \n",
    "    # fit the classifier object using training data\n",
    "    clf.fit(train_data, np.array([label for label in labels.values()]))\n",
    "\n",
    "    count = 0  # the image id\n",
    "    # iterate over the images of the test set\n",
    "    for image, class_name in tqdm(image_generator(testdata_path)):\n",
    "        # extract SIFT features\n",
    "        _, des = sift.detectAndCompute(image, None)\n",
    "        # map each feature to its closest visual word\n",
    "        words, _ = vq(des, vocabulary, check_finite=False)\n",
    "        # build a histogram of visual word occurences inside the test image\n",
    "        histogram = np.histogram(words, bins=np.arange(k + 1))[0].reshape(1, k).astype(np.float64)\n",
    "        # normalize the histogram\n",
    "        histogram /= np.sum(histogram)\n",
    "        # use the classifier to formulate a prediction on the current image\n",
    "        pred = clf.predict(histogram)\n",
    "        # update lists\n",
    "        y_pred.append(int(pred))\n",
    "        y_true.append(classes[class_name])\n",
    "        \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict_linear_svm(sift, vocabulary, train_data, labels, testdata_path, classes, distance=None, kernel=None):\n",
    "    \"\"\"\n",
    "    Fit a linear SVM on a train set and formulate predictions for images in a test set.\n",
    "    In particular, the one-vs-rest approach is used and 15 different classifiers are trained.\n",
    "    @param sift: an object of type cv2.xfeatures2d_SIFT, to extract the SIFT descriptors\n",
    "    @param vocabulary: a NumPy array containing the visual words of the vocabulary\n",
    "    @param train_data: a NumPy array for the train dataset, of size [n_samples, n_features]\n",
    "    @param labels: a dictionary mapping the training images' numbers to their respective class label\n",
    "    @param testdata_path: a string for the directory containing the test images to evaluate\n",
    "    @param std_devs: a NumPy array containing the stds of the training features, of size [1, num_visual_words]\n",
    "    @param classes: a dictionary of class_string: class_number pairs\n",
    "    @return: a tuple (y_true, y_pred), the former being the list of true labels, the latter being the \n",
    "    list of predicted labels\n",
    "    \"\"\"\n",
    "    # allocate lists\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "    # get the number of visual words\n",
    "    k = len(vocabulary)\n",
    "    \n",
    "    # fit 15 one-vs-rest classifier objects using training data\n",
    "    num_classes = len(classes)\n",
    "    classifiers = [SVC(kernel=\"linear\") for _ in range(num_classes)]\n",
    "    curr_label = 0\n",
    "    for clf in classifiers:\n",
    "        # use positive label for the current \"one\" class and negative for the \"rest\" classes\n",
    "        clf = clf.fit(histograms, np.array([1 if label == curr_label else -1 for label in labels.values()]))\n",
    "        curr_label += 1\n",
    "\n",
    "    count = 0  # the image id\n",
    "    # iterate over the images of the test set\n",
    "    for image, class_name in tqdm(image_generator(testdata_path)):\n",
    "        # extract SIFT features\n",
    "        _, des = sift.detectAndCompute(image, None)\n",
    "        # map each feature to its closest visual word\n",
    "        words, _ = vq(des, vocabulary, check_finite=False)\n",
    "        # build a histogram of visual word occurences inside the test image\n",
    "        histogram = np.histogram(words, bins=np.arange(k + 1))[0].reshape(1, k).astype(np.float64)\n",
    "        # normalize the histogram\n",
    "        histogram /= np.sum(histogram)\n",
    "        # use the classifier to formulate a prediction on the current image\n",
    "        # compute the distance from each hyperplane\n",
    "        preds = np.array([np.dot(clf.coef_, histogram.reshape(k, 1)) + clf.intercept_ for clf in classifiers])\n",
    "        # update lists\n",
    "        y_pred.append(np.argmax(preds))  # assign image to the class with the highest real-valued output\n",
    "        y_true.append(classes[class_name])\n",
    "        \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict_kernel_svm(sift, vocabulary, train_data, labels, testdata_path, classes, distance, kernel):\n",
    "    \"\"\"\n",
    "    Fit a non-linear SVM on a train set and formulate predictions for images in a test set.\n",
    "    @param sift: an object of type cv2.xfeatures2d_SIFT, to extract the SIFT descriptors\n",
    "    @param vocabulary: a NumPy array containing the visual words of the vocabulary\n",
    "    @param train_data: a NumPy array for the train dataset, of size [n_samples, n_features]\n",
    "    @param labels: a dictionary mapping the training images' numbers to their respective class label\n",
    "    @param testdata_path: a string for the directory containing the test images to evaluate\n",
    "    @param std_devs: a NumPy array containing the stds of the training features, of size [1, num_visual_words]\n",
    "    @param classes: a dictionary of class_string: class_number pairs\n",
    "    @param distance: distance function to compute the kernel from\n",
    "    @param kernel: kernel function used to perform the \"kernel trick\"\n",
    "    @return: a tuple (y_true, y_pred), the former being the list of true labels, the latter being the \n",
    "    list of predicted labels\n",
    "    \"\"\"\n",
    "    # allocate lists\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "    # get the number of visual words\n",
    "    k = len(vocabulary)\n",
    "    \n",
    "    # pre-compute the Gram matrix among training instances to be\n",
    "    # fed to the \"fit\" function\n",
    "    gram = compute_Gram(train_data, distance, k)\n",
    "    # since this function is supposed to be used with Chi-squared kernel\n",
    "    A = np.mean(gram[np.triu_indices(np.shape(train_data)[0])])  # scale parameter\n",
    "    gram = np.exp(- (gram / A))  # generalized Gaussian kernel\n",
    "    \n",
    "    # fit 15 one-vs-rest classifier objects using training data\n",
    "    num_classes = len(classes)\n",
    "    svc = OneVsRestClassifier(SVC(kernel=\"precomputed\"), n_jobs=-1)\n",
    "    svc = svc.fit(gram, np.array([label for label in labels.values()]))\n",
    "\n",
    "    count = 0  # the image id\n",
    "    # iterate over the images of the test set\n",
    "    for image, class_name in tqdm(image_generator(testdata_path)):\n",
    "        # extract SIFT features\n",
    "        _, des = sift.detectAndCompute(image, None)\n",
    "        # map each feature to its closest visual word\n",
    "        words, _ = vq(des, vocabulary, check_finite=False)\n",
    "        # build a histogram of visual word occurences inside the test image\n",
    "        histogram = np.histogram(words, bins=np.arange(k + 1))[0].reshape(1, k).astype(np.float64)\n",
    "        # normalize the histogram\n",
    "        histogram /= np.sum(histogram)\n",
    "        # use the classifier to formulate a prediction on the current image\n",
    "        pred = svc.predict(np.array([kernel(histogram[0, :], histograms[num, :], k, A) for num in range(num_obs)]).reshape(1, -1))\n",
    "        # update lists\n",
    "        y_pred.append(int(pred))  # assign image to the class with the highest real-valued output\n",
    "        y_true.append(classes[class_name])\n",
    "        \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_assignment(descriptors, sift, labels, testdata_path, classes, count, vocabulary):\n",
    "    \"\"\"\n",
    "    Fit a linear one-vs-rest SVM on a train set and formulate predictions for images in a test set.\n",
    "    Histograms are computed using the soft-assignment rule proposed by Van Gemert (2008).\n",
    "    @param descriptors: a pandas DataFrame containing the train descriptors and metadata\n",
    "    @param sift: an object of type cv2.xfeatures2d_SIFT, to extract the SIFT descriptors\n",
    "    @param labels: a dictionary mapping the training images' numbers to their respective class label\n",
    "    @param testdata_path: a string for the directory containing the test images to evaluate\n",
    "    @param classes: a dictionary of class_string: class_number pairs\n",
    "    @param count: the total number of training images\n",
    "    @param vocabulary: a NumPy array containing the visual words of the vocabulary\n",
    "    @return: a tuple (y_true, y_pred), the former being the list of true labels, the latter being the \n",
    "    list of predicted labels\n",
    "    \"\"\"\n",
    "    # allocate the train histograms\n",
    "    k = len(vocabulary)\n",
    "    histograms = np.zeros((count, k))\n",
    "    # compute kernel invariants; these quantities are fixed for a given sigma in the soft kernel,\n",
    "    # and so are pre-computed for efficiency reasons. In particular, we follow the advice of the\n",
    "    # aforementioned paper to set sigma between 100 and 200\n",
    "    sigma = 150\n",
    "    sigma_2 = 150**2\n",
    "    const = (1 / (sigma * (2 * pi)**(1 / 2)))  # term in front of the Gaussian density\n",
    "\n",
    "    # iterate over the train images and compute the distance-weighted histograms, where each descriptor\n",
    "    # contributes to each bin according to the kernel, and not only to the closest one\n",
    "    for image in tqdm(descriptors[\"image\"].unique()):\n",
    "        desc = np.array([des for des in descriptors[descriptors[\"image\"] == image][\"data\"]])\n",
    "        words = np.array([[soft_kernel(d, word, const, sigma_2) for word in vocabulary] for d in desc])\n",
    "        histograms[image] = np.sum(words, axis=0)\n",
    "        \n",
    "    # normalize histograms\n",
    "    sums = np.sum(histograms, axis=1).reshape(num_obs, 1)\n",
    "    histograms /= sums\n",
    "    \n",
    "    # create and fit a linear SVM to the data\n",
    "    # create and fit a histogram intersection SVM to the data\n",
    "    svc = OneVsRestClassifier(SVC(kernel=\"precomputed\"), n_jobs=-1)\n",
    "    gram = compute_Gram(histograms, histogram_intersection)\n",
    "    svc = svc.fit(gram, np.array([label for label in labels.values()]))\n",
    "\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "\n",
    "    # iterate over the test set\n",
    "    for image, class_name in tqdm(image_generator(testdata_path)):\n",
    "        # extract features\n",
    "        _, desc = sift.detectAndCompute(image, None)\n",
    "        # compute histogram using soft assignment\n",
    "        words = np.array([[soft_kernel(d, word, const, sigma_2) for word in vocabulary] for d in desc])\n",
    "        histogram = np.sum(words, axis=0).reshape(1, k)\n",
    "        # normalize\n",
    "        histogram /= np.sum(histogram)\n",
    "        # predict and update\n",
    "        pred = svc.predict(np.array([histogram_intersection(histogram[0, :], histograms[num, :]) for num in range(num_obs)]).reshape(1, -1))\n",
    "        y_pred.append(int(pred))\n",
    "        y_true.append(classes[class_name])\n",
    "        \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_pyramid_kernel(descriptors, sift, labels, classes, count, vocabulary, paths, levels=3, weights={0: np.array([0.25]), 1: np.array([0.15]), 2: np.array([0.5])}):\n",
    "    \"\"\"\n",
    "    Fit a linear one-vs-rest SVM on a train set and formulate predictions for images in a test set.\n",
    "    The algorithm followed is the Spatial Pyramid Kernel proposed by Lazebnik et al., 2006.\n",
    "    @param descriptors: a pandas DataFrame containing the train descriptors and metadata\n",
    "    @param sift: an object of type cv2.xfeatures2d_SIFT, to extract the SIFT descriptors\n",
    "    @param labels: a dictionary mapping the training images' numbers to their respective class label\n",
    "    @param classes: a dictionary of class_string: class_number pairs\n",
    "    @param count: the total number of training images\n",
    "    @param vocabulary: a NumPy array containing the visual words of the vocabulary\n",
    "    @param paths: a tuple of strings for the directories containing the training and test images\n",
    "    @param levels: the number of levels of the SPK\n",
    "    @param weights: a dictionary mapping each level to its weight (a scalar NumPy array)\n",
    "    @return: a tuple (y_true, y_pred), the former being the list of true labels, the latter being the \n",
    "    list of predicted labels\n",
    "    \"\"\"\n",
    "    # unpack paths, get number of visual words\n",
    "    traindata_path, testdata_path = paths\n",
    "    k = len(vocabulary)\n",
    "    # compute total number of features across all the subregions\n",
    "    ext_k = k * sum([4**l for l in range(levels)])\n",
    "    \n",
    "    # allocate \"extended\" histograms\n",
    "    histograms = np.zeros((count, ext_k))\n",
    "\n",
    "    # compute extended histograms\n",
    "    count = 0\n",
    "    for image, _ in tqdm(image_generator(traindata_path)):\n",
    "        start = 0\n",
    "        a, b, _ = np.shape(image)\n",
    "        # for each level of the pyramid\n",
    "        for l in range(levels):\n",
    "            # get width and height of the subregions\n",
    "            x_step = floor(a / (2**l))\n",
    "            y_step = floor(b / (2**l))\n",
    "            w = weights[l]\n",
    "            # for each subregion of the current level\n",
    "            for _ in range(1, 2**l + 1):\n",
    "                x = 0\n",
    "                for _ in range(1, 2**l + 1):\n",
    "                    y = 0\n",
    "                    # extract descriptors considering only the subregion\n",
    "                    _, des = sift.detectAndCompute(image[x:x+x_step, y:y+y_step], None)\n",
    "                    # there might be some subregions having no descriptors; just fill the corresponding\n",
    "                    # histogram with zeros and move to the next iteration\n",
    "                    if not len(_):\n",
    "                        histograms[count, start:start+k] = np.zeros((1, k))\n",
    "                        start += k\n",
    "                        y += y_step\n",
    "                        continue\n",
    "                    # quantize descriptors\n",
    "                    words, _ = vq(des, vocabulary, check_finite=False)\n",
    "                    # concatenate current histogram\n",
    "                    histograms[count, start:start+k], _ = w * np.histogram(words, bins=np.arange(k + 1))\n",
    "                    start += k\n",
    "                    y += y_step\n",
    "                x += x_step\n",
    "        count += 1\n",
    "    \n",
    "    # normalize histograms\n",
    "    sums = np.sum(histograms, axis=1).reshape(num_obs, 1)\n",
    "    histograms /= sums\n",
    "    \n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "    \n",
    "    # create and fit a histogram intersection SVM to the data\n",
    "    svc = OneVsRestClassifier(SVC(kernel=\"precomputed\"), n_jobs=-1)\n",
    "    gram = compute_Gram(histograms, histogram_intersection)\n",
    "    svc = svc.fit(gram, np.array([label for label in labels.values()]))\n",
    "\n",
    "    # for each test image, perform prediction pipeline\n",
    "    for image, class_name in tqdm(image_generator(testdata_path)):\n",
    "        a, b, _ = np.shape(image)\n",
    "        histogram = np.zeros((1, ext_k))\n",
    "        start = 0\n",
    "        # for each level of the pyramid\n",
    "        for l in range(levels):\n",
    "            # get height and width of the subregions\n",
    "            x_step = floor(a / (2**l))\n",
    "            y_step = floor(b / (2**l))\n",
    "            w = weights[l]\n",
    "            # for each subregion of the current level\n",
    "            for _ in range(1, 2**l + 1):\n",
    "                x = 0\n",
    "                for _ in range(1, 2**l + 1):\n",
    "                    y = 0\n",
    "                    # extract descriptors from the subregion\n",
    "                    _, des = sift.detectAndCompute(image[x:x+x_step, y:y+y_step], None)\n",
    "                    # there might be regions with no descriptors computed; just fill the histogram\n",
    "                    # with zeros and move to the next iteration\n",
    "                    if not len(_):\n",
    "                        histogram[0, start:start+k] = np.zeros((1, k))\n",
    "                        start += k\n",
    "                        y += y_step\n",
    "                        continue\n",
    "                    # quantize the descriptors into histograms of visual words and concatenate\n",
    "                    words, _ = vq(des, kmeans.cluster_centers_, check_finite=False)\n",
    "                    histogram[0, start:start+k], _ = w * np.histogram(words, bins=np.arange(k + 1))\n",
    "                    start += k\n",
    "                    y += y_step\n",
    "                x += x_step\n",
    "        # normalize\n",
    "        histogram /= np.sum(histogram)\n",
    "        # predict using a histogram intersection kernel\n",
    "        pred = svc.predict(np.array([histogram_intersection(histogram[0, :], histograms[num, :]) for num in range(num_obs)]).reshape(1, -1))\n",
    "        # update\n",
    "        y_pred.append(int(pred))\n",
    "        y_true.append(classes[class_name])\n",
    "        \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Construct a confusion matrix.\n",
    "    @param y_true: a list with the true labels for each test example\n",
    "    @param y_pred: a list with the predicted labels for each test example\n",
    "    @param num_classes: the total number of different classes\n",
    "    @return: a NumPy array for the confusion matrix, of size [num_classes, num_classes]\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((num_classes, num_classes))\n",
    "    for num in range(len(y_true)):\n",
    "        matrix[y_true[num], y_pred[num]] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_matrix, classes, name=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Produce a plot for a given confusion matrix, normalizing with respect to the true\n",
    "    labels, with the true class on the y-axis and the predicted class on the x-axis.\n",
    "    @param confusion_matrix: the confusion matrix to plot\n",
    "    @param classes: a dictionary of class_string: class_number pairs\n",
    "    @param name: the name used to save the plot, including the extension\n",
    "    @param cmap: the colormap to use when plotting\n",
    "    @return: None, just plot\n",
    "    \"\"\"\n",
    "    confusion_matrix /= np.sum(confusion_matrix, axis=1)\n",
    "    plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(\"normalized confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    marks = np.arange(len(classes))\n",
    "    plt.xticks(marks, list(classes.keys()), rotation=90)\n",
    "    plt.yticks(marks, list(classes.keys()))\n",
    "    plt.xlabel(\"predicted class\")\n",
    "    plt.ylabel(\"true class\")\n",
    "    if name:\n",
    "        plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Compute the accuracy on a dataset from a confusion matrix, as the\n",
    "    sum of the diagonal entries.\n",
    "    @param confusion_matrix: the output of the evaluation of a classifier on a test set\n",
    "    @return: the accuracy as a float\n",
    "    \"\"\"\n",
    "    return np.sum(np.diag(confusion_matrix)) / np.sum(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CELL\n",
    "\n",
    "# paths and important global variables\n",
    "traindata_path = \"./data/train\"\n",
    "testdata_path = \"./data/test\" \n",
    "k = 50\n",
    "n_sampled = 100000\n",
    "classes = {os.listdir(traindata_path)[num]: num - 1 for num in range(1, len(os.listdir(traindata_path)))}\n",
    "\n",
    "# compute descriptors\n",
    "descriptors = get_descriptors(traindata_path)\n",
    "# quantize descriptors into visual words\n",
    "kmeans = get_visual_words(descriptors, k, n_sampled, descriptors.shape[0])\n",
    "# compute histograms\n",
    "histograms = get_histograms(descriptors, count, kmeans.cluster_centers_)\n",
    "num_obs, _ = np.shape(histograms)\n",
    "bins = np.arange(k + 1)\n",
    "# normalize\n",
    "sums = np.sum(histograms, axis=1).reshape(num_obs, 1)\n",
    "histograms /= sums\n",
    "\n",
    "# RUN TESTS: UNCOMMENT/COMMENT WHERE NECESSARY\n",
    "# run KNN test\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric=histEMD, n_jobs=-1)\n",
    "y_true, y_pred = fit_and_predict_knn(knn, sift, kmeans3.cluster_centers_, histograms, labels, testdata_path, classes)\n",
    "# run linear SVM test\n",
    "#y_true, y_pred = fit_and_predict_linear_svm(sift, kmeans.cluster_centers_, histograms, labels, testdata_path, classes)\n",
    "# run chisquared SVM test\n",
    "#y_true, y_pred = fit_and_predict_kernel_svm(sift, kmeans.cluster_centers_, histograms, labels, testdata_path, classes, distance=chisquared_distance, kernel=chisquared_kernel)\n",
    "# run soft assignment test\n",
    "#y_true, y_pred = soft_assignment(descriptors, sift, labels, testdata_path, classes, count, kmeans.cluster_centers_)\n",
    "# run SPK test\n",
    "#y_true, y_pred = spatial_pyramid_kernel(descriptors, sift, labels, classes, count, kmeans.cluster_centers_, (traindata_path, testdata_path))\n",
    "\n",
    "# compute confusion matrix and visualize results\n",
    "cm = build_confusion_matrix(y_true, y_pred, len(classes))\n",
    "print(accuracy(cm))\n",
    "plot_confusion_matrix(cm, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
